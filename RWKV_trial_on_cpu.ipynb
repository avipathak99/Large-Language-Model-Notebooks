{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VtIzUxEFu35b",
    "outputId": "e0b5977d-e503-40b1-faa6-fec65857f2a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'RWKV-LM'...\n",
      "remote: Enumerating objects: 1815, done.\u001b[K\n",
      "remote: Counting objects: 100% (810/810), done.\u001b[K\n",
      "remote: Compressing objects: 100% (143/143), done.\u001b[K\n",
      "remote: Total 1815 (delta 763), reused 672 (delta 667), pack-reused 1005\u001b[K\n",
      "Receiving objects: 100% (1815/1815), 11.40 MiB | 679.00 KiB/s, done.\n",
      "Resolving deltas: 100% (1151/1151), done.\n",
      "--2023-06-30 21:25:07--  https://huggingface.co/BlinkDL/rwkv-4-pile-1b5/resolve/main/RWKV-4-Pile-1B5-20220903-8040.pth\n",
      "Resolving huggingface.co (huggingface.co)... 2600:9000:2079:1400:17:b174:6d00:93a1, 2600:9000:2079:9800:17:b174:6d00:93a1, 2600:9000:2079:7e00:17:b174:6d00:93a1, ...\n",
      "Connecting to huggingface.co (huggingface.co)|2600:9000:2079:1400:17:b174:6d00:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.huggingface.co/repos/d6/95/d69583b06567422d104d5413e7926ae97bcf0d541619db6e61fe10133d91582d/4e215be3b4f86dc2f145835b47a2c432306c373cbf625375b7721bb474512bad?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-4-Pile-1B5-20220903-8040.pth%3B+filename%3D%22RWKV-4-Pile-1B5-20220903-8040.pth%22%3B&Expires=1688399709&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Q2Lzk1L2Q2OTU4M2IwNjU2NzQyMmQxMDRkNTQxM2U3OTI2YWU5N2JjZjBkNTQxNjE5ZGI2ZTYxZmUxMDEzM2Q5MTU4MmQvNGUyMTViZTNiNGY4NmRjMmYxNDU4MzViNDdhMmM0MzIzMDZjMzczY2JmNjI1Mzc1Yjc3MjFiYjQ3NDUxMmJhZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgzOTk3MDl9fX1dfQ__&Signature=BTNz5W0Ufk30tIsA%7EIojbW4rmH4OiYmGSScmA9uAAyKmcJUtRRrQ3JZCMBRHIgRgqUVqdcoX%7EQAvATNTWAqfsn%7Ey8BOn8p0MWQETU7ZeavKgnEqz9vTVgRpnVBl5%7Evo6PZoV3esK9lPwUQ1EheyI14xgvr7zl2xZxLqR2qAqpc74Ukpffr1Qzl12j5OrYOk9Il9k6H97JdFF-mXY7DRwoTALtr02ZThCoyv6DjNUuaEbn-Zo5OV7NLEOaffHLx22qUAHbtGokwkaxHrUOsdUBGfuzydlPsAoQDb-WZdGPM4iXlLiZimuXqiM0C8pPgXBtKsOpIJ0fkrjdf8wuoI2aA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
      "--2023-06-30 21:25:08--  https://cdn-lfs.huggingface.co/repos/d6/95/d69583b06567422d104d5413e7926ae97bcf0d541619db6e61fe10133d91582d/4e215be3b4f86dc2f145835b47a2c432306c373cbf625375b7721bb474512bad?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27RWKV-4-Pile-1B5-20220903-8040.pth%3B+filename%3D%22RWKV-4-Pile-1B5-20220903-8040.pth%22%3B&Expires=1688399709&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2Q2Lzk1L2Q2OTU4M2IwNjU2NzQyMmQxMDRkNTQxM2U3OTI2YWU5N2JjZjBkNTQxNjE5ZGI2ZTYxZmUxMDEzM2Q5MTU4MmQvNGUyMTViZTNiNGY4NmRjMmYxNDU4MzViNDdhMmM0MzIzMDZjMzczY2JmNjI1Mzc1Yjc3MjFiYjQ3NDUxMmJhZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODgzOTk3MDl9fX1dfQ__&Signature=BTNz5W0Ufk30tIsA%7EIojbW4rmH4OiYmGSScmA9uAAyKmcJUtRRrQ3JZCMBRHIgRgqUVqdcoX%7EQAvATNTWAqfsn%7Ey8BOn8p0MWQETU7ZeavKgnEqz9vTVgRpnVBl5%7Evo6PZoV3esK9lPwUQ1EheyI14xgvr7zl2xZxLqR2qAqpc74Ukpffr1Qzl12j5OrYOk9Il9k6H97JdFF-mXY7DRwoTALtr02ZThCoyv6DjNUuaEbn-Zo5OV7NLEOaffHLx22qUAHbtGokwkaxHrUOsdUBGfuzydlPsAoQDb-WZdGPM4iXlLiZimuXqiM0C8pPgXBtKsOpIJ0fkrjdf8wuoI2aA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
      "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 2600:9000:256c:3400:11:f807:5180:93a1, 2600:9000:256c:d400:11:f807:5180:93a1, 2600:9000:256c:3600:11:f807:5180:93a1, ...\n",
      "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|2600:9000:256c:3400:11:f807:5180:93a1|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3030279587 (2.8G) [application/zip]\n",
      "Saving to: ‘./RWKV-LM/RWKV-v4/500.pth’\n",
      "\n",
      "./RWKV-LM/RWKV-v4/5 100%[===================>]   2.82G  2.09MB/s    in 39m 17s \n",
      "\n",
      "2023-06-30 22:04:28 (1.23 MB/s) - ‘./RWKV-LM/RWKV-v4/500.pth’ saved [3030279587/3030279587]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/BlinkDL/RWKV-LM.git\n",
    "#!wget https://huggingface.co/BlinkDL/rwkv-4-pile-3b/resolve/main/RWKV-4-Pile-3B-20220915-1207.pth -O ./RWKV-LM/RWKV-v4/500.pth 3B needs more vram then google offers\n",
    "!wget https://huggingface.co/BlinkDL/rwkv-4-pile-1b5/resolve/main/RWKV-4-Pile-1B5-20220903-8040.pth -O ./RWKV-LM/RWKV-v4/500.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g5JdHvg2zjom",
    "outputId": "ee97a282-f748-439a-eccb-ae95aa6adeab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/pathakav/chatRKWV/RWKV-LM/RWKV-v4\n"
     ]
    }
   ],
   "source": [
    "%cd ./RWKV-LM/RWKV-v4/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F02bGykkyicB",
    "outputId": "d20f4c53-3e72-4a7b-f64f-6deadcf40714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/pathakav/my_python/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: requests in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/pathakav/my_python/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec in /Users/pathakav/my_python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/pathakav/my_python/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pathakav/my_python/lib/python3.11/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pathakav/my_python/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pathakav/my_python/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pathakav/my_python/lib/python3.11/site-packages (from requests->transformers) (2023.5.7)\n",
      "Collecting ninja\n",
      "  Using cached ninja-1.11.1-py2.py3-none-macosx_10_9_universal2.macosx_10_9_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (270 kB)\n",
      "Installing collected packages: ninja\n",
      "Successfully installed ninja-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install ninja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7P7ISTa1XgC",
    "outputId": "09e3d1f8-3977-44d4-9912-42515e43825b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 500...\n",
      "\n",
      "RWKV_HEAD_QK_DIM 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pathakav/my_python/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math, os\n",
    "import time\n",
    "import types\n",
    "import copy\n",
    "import torch\n",
    "from torch.nn import functional as F\n",
    "from src.utils import TOKENIZER, Dataset\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "np.set_printoptions(precision=4, suppress=True, linewidth=200)\n",
    "\n",
    "\n",
    "TOKEN_MODE = 'pile' # char / bpe / pile\n",
    "\n",
    "n_layer = 6\n",
    "n_embd = 512\n",
    "ctx_len = 10024\n",
    "\n",
    "if TOKEN_MODE == 'char':\n",
    "    MODEL_NAME = 'trained-500'  # your trained model\n",
    "    WORD_NAME = 'vocab'         # the .json vocab (generated by train.py)\n",
    "    # set UNKNOWN_CHAR to the rarest token in your vocab.json, and all unknown tokens in your prompt will be denoted by it\n",
    "    UNKNOWN_CHAR = ' '          # here we just set it to ' ' for simplicity\n",
    "\n",
    "elif TOKEN_MODE == 'bpe':\n",
    "    MODEL_NAME = 'trained-500'  # your trained model\n",
    "    WORD_NAME = ['model-vocab.json', 'model-merges.txt'] # [vocab, merge] for your BPE model\n",
    "    UNKNOWN_CHAR = None\n",
    "\n",
    "elif TOKEN_MODE == 'pile':\n",
    "    WORD_NAME = ['20B_tokenizer.json', '20B_tokenizer.json']\n",
    "    UNKNOWN_CHAR = None\n",
    "\n",
    "    #---> you can set MODEL_NAME to your fine-tuned model <---\n",
    "\n",
    "    MODEL_NAME = '500'\n",
    "\n",
    "    # for 3b\n",
    "    #n_layer = 32\n",
    "    #n_embd = 2560\n",
    "    #ctx_len = 10024\n",
    "\n",
    "    # for 1b5'\n",
    "    n_layer = 24\n",
    "    n_embd = 2048\n",
    "    ctx_len = 1024\n",
    "\n",
    "os.environ['RWKV_FLOAT_MODE'] = 'bf16'  # 'bf16' / 'fp16' / 'fp32' (note: only using fp32 at this moment)\n",
    "os.environ['RWKV_RUN_DEVICE'] = 'cpu'   # 'cpu' (already very fast) or 'cuda'\n",
    "model_type = 'RWKV' # 'RWKV' or 'RWKV-ffnPre'\n",
    "\n",
    "\n",
    "NUM_TRIALS = 5\n",
    "LENGTH_PER_TRIAL = 3330\n",
    "\n",
    "DEBUG_DEBUG = False  # True False --> show softmax output\n",
    "\n",
    "print(f'Loading {MODEL_NAME}...')\n",
    "from src.model_run import RWKV_RNN\n",
    "model = RWKV_RNN(MODEL_NAME, os.environ['RWKV_RUN_DEVICE'], model_type, n_layer, n_embd, ctx_len)\n",
    "tokenizer = TOKENIZER(WORD_NAME, UNKNOWN_CHAR=UNKNOWN_CHAR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Qw1y-qAwTzU",
    "outputId": "86e8b463-7d22-417c-e9d9-e4826d76a4ac"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Co9eLstRwRZ_",
    "outputId": "8f7f17ac-2439-4d6a-b845-564c09bc9b59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your prompt has 21 tokens.\n",
      "\n",
      "--> Currently the first run takes a while if your prompt is long, as we are using RNN to process the prompt. Use GPT to build the hidden state for better speed. <--\n",
      "\n",
      "------------------------------Switzerland is beautiful, clean, green, and has very good infrastructure for travel, a heaven for tourism. The easiest route is from Switzerland to England, and then take the quick train from there to Berlin. You will get a decent layover in Germany. That is probably the cheapest way to go.\n",
      "\n",
      "For non-Europeans, please use the right-of-way. I have taken an Estonian train Regarding the map: I believe the width of the track is not great for a train. You should definitely not take a ticket from Tallinn to Tallinn, and then from Tall\n",
      "---------- 38.26s "
     ]
    }
   ],
   "source": [
    "\n",
    "context = \"Switzerland is beautiful, clean, green, and has very good infrastructure for travel, a heaven for tourism\"\n",
    "\n",
    "NUM_TRIALS = 1\n",
    "LENGTH_PER_TRIAL = 100\n",
    "\n",
    "TEMPERATURE = 0.9\n",
    "top_p = 0.8\n",
    "top_p_newline = 0.9 # only used in TOKEN_MODE = char\n",
    "\n",
    "if tokenizer.charMode:\n",
    "    context = tokenizer.refine_context(context)\n",
    "    ctx = [tokenizer.stoi.get(s, tokenizer.UNKNOWN_CHAR) for s in context]\n",
    "else:\n",
    "    ctx = tokenizer.tokenizer.encode(context)\n",
    "src_len = len(ctx)\n",
    "src_ctx = ctx.copy()\n",
    "\n",
    "print('\\nYour prompt has ' + str(src_len) + ' tokens.')\n",
    "print('\\n--> Currently the first run takes a while if your prompt is long, as we are using RNN to process the prompt. Use GPT to build the hidden state for better speed. <--\\n')\n",
    "\n",
    "for TRIAL in range(1 if DEBUG_DEBUG else NUM_TRIALS):\n",
    "    t_begin = time.time_ns()\n",
    "    print(('-' * 30) + context, end='')\n",
    "    ctx = src_ctx.copy()\n",
    "    model.clear()\n",
    "    if TRIAL == 0:\n",
    "        init_state = types.SimpleNamespace()\n",
    "        for i in range(src_len):\n",
    "            x = ctx[:i+1]\n",
    "            if i == src_len - 1:\n",
    "                init_state.out = model.run(x)\n",
    "            else:\n",
    "                model.run(x)\n",
    "        model.save(init_state)\n",
    "    else:\n",
    "        model.load(init_state)\n",
    "\n",
    "    for i in range(src_len, src_len + (1 if DEBUG_DEBUG else LENGTH_PER_TRIAL)):\n",
    "        x = ctx[:i+1]\n",
    "        x = x[-ctx_len:]\n",
    "\n",
    "        if i == src_len:\n",
    "            out = copy.deepcopy(init_state.out)\n",
    "        else:\n",
    "            out = model.run(x)\n",
    "        if DEBUG_DEBUG:\n",
    "            print('model', np.array(x), '==>', np.array(\n",
    "                out), np.max(out), np.min(out))\n",
    "\n",
    "        if TOKEN_MODE == 'pile':\n",
    "            out[0] = -999999999  # disable <|endoftext|>\n",
    "\n",
    "        char = tokenizer.sample_logits(out, x, ctx_len, temperature=TEMPERATURE,\n",
    "                                       top_p_usual=top_p, top_p_newline=top_p_newline)\n",
    "        char = char.item()\n",
    "        if tokenizer.charMode:\n",
    "            print(tokenizer.itos[int(char)], end='', flush=True)\n",
    "        else:\n",
    "            print(tokenizer.tokenizer.decode(int(char)), end='', flush=True)\n",
    "        ctx += [char]\n",
    "\n",
    "    t_end = time.time_ns()\n",
    "    print(\"\\n----------\", round((t_end - t_begin) / (10 ** 9), 2), end='s ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
